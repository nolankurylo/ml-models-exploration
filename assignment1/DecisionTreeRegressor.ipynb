{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SENG 474\n",
    "# Assignment 1 - Problem 6\n",
    "# Nolan Kurylo\n",
    "# V00893175\n",
    "References:\n",
    "\n",
    "1) https://pandas.pydata.org/pandas-docs/stable/reference/index.html\n",
    "\n",
    "2) https://www.python-course.eu/Regression_Trees.php\n",
    "\n",
    "3) https://machinelearningmastery.com/implement-resampling-methods-scratch-python/\n",
    "\n",
    "\n",
    "Please Note: very similar code to that from Problem 2, the only difference is that the current node to be split on in tree is decided by the feature with the minimum variance (functions: min_variance() and weighted_variance()) as compared to Problem 2 that splits the current node on the feature with the largest information gain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RegressionTree . . . (this will take ~10 seconds)\n",
      "The Training root mean squared error (RMSE) is: 0.03398957810128662\n",
      "The Validation root mean squared error (RMSE) is: 0.032298402433508705\n"
     ]
    }
   ],
   "source": [
    "# 6.1 Regress\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pprint\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "np.random.seed(1337)\n",
    "\n",
    "df = pd.read_csv('elections_clean.csv') # create dataframe of csv\n",
    "\n",
    "label_vector = df.pop('PovertyLevel') # this is the target/label vector\n",
    "\n",
    "feature_list = ['Education','Religion','EthnicMale','EthnicFemale', 'PerCapitaInc'] # these will be the features in the tree\n",
    "df = df[feature_list]\n",
    "\n",
    "df['PovertyLevel'] = label_vector # place label vector at the end of df\n",
    "\n",
    "\n",
    "def weighted_variance(dataset, col):\n",
    "    \"\"\" Finds the weighted variance of the subset dataset for the input column col\n",
    "    :param dataset: subset of original dataframe\n",
    "    :param col: col to be calculated on\n",
    "    :return: weighted variance for the column\n",
    "    \"\"\"\n",
    "    values, num_values = np.unique(col, return_counts=True) \n",
    "    weighted_var = 0\n",
    "    for i in range(len(values)):\n",
    "        matching_rows = dataset.loc[col==values[i]] # In the column col, find all rows with matching value values[i] \n",
    "        label_vector_subset = matching_rows['PovertyLevel'] # return a subset of the values in the label vector\n",
    "        weight = len(label_vector_subset) / len(dataset) # weight is the fraction of the entire dataset that the subset consumes\n",
    "        weighted_var += (weight * label_vector_subset.var())\n",
    "    return weighted_var\n",
    "\n",
    " \n",
    "def min_variance(dataset, feature_list): \n",
    "    \"\"\" Finds the feature still in the feature_list with the minimum variance (feature to be split on)\n",
    "    :param dataset: subset of original dataframe\n",
    "    :param feature_list: subset of features from the features in the dataset \n",
    "    :return: feature with the lowest variance from the feature_list\n",
    "    \"\"\"\n",
    "    min_variance = {}\n",
    "\n",
    "    for feature in feature_list: # add variances to dictionary for each feature still in thje feature_list\n",
    "        feature_variance = weighted_variance(dataset, dataset[feature])\n",
    "        min_variance[feature] = feature_variance\n",
    "    \n",
    "    return min(min_variance, key=min_variance.get)\n",
    "\n",
    "\n",
    "def RegressionID3(dataset, feature_list, parent=None): \n",
    "    \"\"\" Recursively builds a dictionary-structured Decision Tree Regressor based on ID3 algorithm\n",
    "    :param dataset: training set\n",
    "    :param feature_list: list of all features from the dataset\n",
    "    :param parent: most common element in the current label vector subset\n",
    "    :return: Decision Tree structured as a dictionary\n",
    "    \"\"\"\n",
    "\n",
    "    if len(np.unique(dataset[\"PovertyLevel\"])) == 1: # if the label vector is \"pure\" (only one value in label vector)\n",
    "        return dataset[\"PovertyLevel\"][0] # return the pure value and go back up tree\n",
    "\n",
    "    if(len(feature_list) == 0): # reached the end of the branch\n",
    "        return parent # return parent value and go back up tree\n",
    "\n",
    "    if(len(dataset) == 0): # if dirty split, return the original mean of the label vector\n",
    "        return df['PovertyLevel'].mean()\n",
    "\n",
    "   \n",
    "    curr_node = min_variance(dataset, feature_list) # find the feature with the min variance and make it the curr_node in the tree\n",
    "\n",
    "    curr_node_values, num_values = np.unique(dataset[curr_node], return_counts=True) # get the values and their counts associated with the curr_node\n",
    "    \n",
    "    parent = dataset['PovertyLevel'].mode()[0] # make parent the most common element in the current label vector subset\n",
    "    \n",
    "\n",
    "    DecisionTree = {curr_node:{}}\n",
    "\n",
    "    feature_list = list(filter(lambda feature: feature != curr_node, feature_list)) # remove the curr_node (.remove() was causing issues)\n",
    "    \n",
    "\n",
    "    for value in curr_node_values: # for each value associated with the curr_node feature\n",
    "        \n",
    "        sub_dataset = dataset[dataset[curr_node] == value].reset_index(drop=True) # split data into a subset that excludes all rows of the current 'value' of the curr_node in the loop\n",
    "        DecisionTree[curr_node][value] = RegressionID3(sub_dataset, feature_list, parent) # recursively build the tree on the new subset as the dataset\n",
    "           \n",
    "    return DecisionTree\n",
    "\n",
    "\n",
    "def split_training_validation_sets(df):\n",
    "    \"\"\" Find 70% of the original dataset as the training set, 30% as the validaiton set\n",
    "    :param df: dataframe to be split\n",
    "    :return: training and validation splits as dataframes\n",
    "    \"\"\"\n",
    "    shuffled_dataset = df.sample(frac=1).reset_index(drop=True) # shuffle the dataset\n",
    "    \n",
    "    split_70_30 = int(df.shape[0] * 0.7) # find the index for the 70 / 30 split to split on\n",
    "\n",
    "    \n",
    "    training_set = shuffled_dataset.iloc[:split_70_30].reset_index(drop=True) # 70% of dataset\n",
    "    validation_set = shuffled_dataset.iloc[split_70_30:].reset_index(drop=True) # 30% of dataset\n",
    "    return training_set,validation_set\n",
    "\n",
    "def predict(validation_sample, DecisionTree): \n",
    "    \"\"\" Recursively runs through DecisionTree and returns where the input validation_sample evaluates a continous value\n",
    "    :param validation_sample: sample for the Decision tree to predict\n",
    "    :param DecisionTree: dictionary structured model\n",
    "    :return: the binary prediction - 0 or 1\n",
    "    \"\"\"\n",
    "    \n",
    "    for feature in list(validation_sample.keys()): # for each feature in the sample\n",
    "        \n",
    "        if feature not in list(DecisionTree.keys()): #ignore missing features\n",
    "            continue\n",
    "        try:\n",
    "            prediction = DecisionTree[feature][validation_sample[feature]] # see if node in tree is a leaf( 0 or 1) or a dict (sub tree)\n",
    "            \n",
    "            \n",
    "            if(type(prediction) == float): # node is a leaf, return the label value\n",
    "                \n",
    "                return prediction\n",
    "            else: # node is a subtree, recursively go down to next node in subtree\n",
    "                \n",
    "                return predict(validation_sample, prediction)\n",
    "\n",
    "        except:\n",
    "            return df['PovertyLevel'].mean()\n",
    "        \n",
    "\n",
    "\n",
    "def find_rmse(validation_set, DecisionTree): \n",
    "    \"\"\" Finds the Root Mean Squared Error (RMSE) of the DecsionTree given an input validaetion_set and the DecisionTree\n",
    "    :param validation_set: samples for the Decision tree to predict\n",
    "    :param DecisionTree: dictionary structured model\n",
    "    return: root mean squared error (RMSE)\n",
    "    \"\"\"\n",
    "    validation_samples = validation_set.iloc[:,:-1].to_dict(orient='records') # convert dataset to dictionary\n",
    "    \n",
    "    actual_output = validation_set.iloc[:,-1] # remove the label vector so we can see how how well the tree predicts the samples\n",
    "    validation_set['predicted_output'] = actual_output # create empty column for predictions\n",
    "    predicted_output = np.array([]) # array for predictions \n",
    "    num_correct = 0 # store the number of samples in the validation_set that are actually correct\n",
    "    for i in range(len(validation_samples)):\n",
    "        prediction = predict(validation_samples[i],DecisionTree)\n",
    "        predicted_output = np.append(predicted_output, prediction) #update prediction array\n",
    "    \n",
    "    sum_arg = ((actual_output-predicted_output)**2)/len(actual_output) # argument of summation\n",
    "    mse = sum_arg.sum() # mean sqaured error\n",
    "    rmse = np.sqrt(mse) # root of mean squared error\n",
    "    return rmse \n",
    "\n",
    "\n",
    "\n",
    "training_set, validation_set = split_training_validation_sets(df) # split dataset into %70 training 30% validation sets\n",
    " \n",
    "print(\"Training RegressionTree . . . (this will take ~10 seconds)\")\n",
    "DecisionTree = RegressionID3(training_set, feature_list) # train the tree\n",
    "\n",
    "training_rmse = find_rmse(training_set, DecisionTree)  # find the root mean squared error\n",
    "validation_rmse = find_rmse(validation_set, DecisionTree)  # find the root mean squared error\n",
    "print(\"The Training root mean squared error (RMSE) is: \" + str(training_rmse))\n",
    "print(\"The Validation root mean squared error (RMSE) is: \" + str(validation_rmse))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training RegressionTree with Validation Fold 1 . . . (this will take ~10 seconds)\n",
      "RegressionTree using Validation Fold 1 RMSE: 0.033110431956701984\n",
      "Training RegressionTree with Validation Fold 2 . . . (this will take ~10 seconds)\n",
      "RegressionTree using Validation Fold 2 RMSE: 0.03354747717124353\n",
      "Training RegressionTree with Validation Fold 3 . . . (this will take ~10 seconds)\n",
      "RegressionTree using Validation Fold 3 RMSE: 0.033558751396049406\n",
      "Training RegressionTree with Validation Fold 4 . . . (this will take ~10 seconds)\n",
      "RegressionTree using Validation Fold 4 RMSE: 0.03353583320056681\n",
      "Training RegressionTree with Validation Fold 5 . . . (this will take ~10 seconds)\n",
      "RegressionTree using Validation Fold 5 RMSE: 0.033699187258665886\n",
      "\n",
      "After 5-Fold cross-validation, the best RegressionTree comes from using Validation Fold 1 giving an RMSE of 0.033110431956701984\n"
     ]
    }
   ],
   "source": [
    "# 6.2 Cross-Validate\n",
    "import random \n",
    "\n",
    "def cross_validate(data):\n",
    "    \"\"\" Creates list of 5 folds (datasets)\n",
    "    :param data: original dataframe\n",
    "    :return: list of the 5 folds\n",
    "    \"\"\"\n",
    "    folds = []\n",
    "    dataset = data\n",
    "    num_folds = 5 \n",
    "    fold_len = int(len(dataset) / num_folds)\n",
    "    \n",
    "    for i in range(num_folds):\n",
    "        fold = [] # list to store each of the 5 fold datasets\n",
    "        while len(fold) < fold_len:\n",
    "            index = random.randrange(len(dataset)) # choose a randomized index\n",
    "            row = dataset.iloc[index] #save the row\n",
    "            dataset = dataset.drop(dataset.index[index]).reset_index(drop=True) # remove chosen index from dataset\n",
    "            fold.append(row) # add the chosen index (row) to the current fold dataset \n",
    "        folds.append(fold)\n",
    "    return folds \n",
    "\n",
    "def findBestFold(folds, feature_list):\n",
    "    \"\"\" Finds the fold with the minimum rmse value and the fold (iteration) number\n",
    "    :param folds: list of 5 folds\n",
    "    :param feature_list: list of all features from the dataset\n",
    "    :return: fold with min rmse and the fold number\n",
    "    \"\"\"\n",
    "    for i in range(len(folds)):\n",
    "        folds[i] = pd.DataFrame(folds[i]) # convert to dataframe\n",
    "\n",
    "    iterations = {} # to store the rmse of each iteration\n",
    "    for i in range(len(folds)):  \n",
    "\n",
    "        validation_fold = folds.pop(i) # store validation fold for current iteration\n",
    "        training_folds = pd.concat(folds) # create training fold of all other folds\n",
    "        folds.insert(0, validation_fold) # put validation fold back in so it can be used as a training fold later\n",
    "\n",
    "        print(\"Training RegressionTree with Validation Fold \" + str(i+1) + \" . . . (this will take ~10 seconds)\")\n",
    "        DecisionTreei = RegressionID3(training_folds, feature_list)\n",
    "        rmse = find_rmse(validation_fold, DecisionTreei) \n",
    "        iterations[\"Validation Fold \" + str(i+1)] = rmse\n",
    "        print(\"RegressionTree using Validation Fold \" + str(i+1) + \" RMSE: \" + str(rmse))\n",
    "        \n",
    "    \n",
    "    min_iteration = min(iterations, key=iterations.get) # find the iteration with the tree that produces the (best) minimzed error\n",
    "    return min_iteration, iterations[min_iteration]\n",
    "\n",
    "\n",
    "folds = cross_validate(df) # get a list of 5 folds to be used for finding the best regression tree\n",
    "\n",
    "bestFold, bestFoldRMSE = findBestFold(folds, feature_list) # algorithm from scratch to find the fold that produces a regression tree with the best (minimzed) error\n",
    "print()\n",
    "print(\"After 5-Fold cross-validation, the best RegressionTree comes from using \" + bestFold + \" giving an RMSE of \" + str(bestFoldRMSE))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "name": "python390jvsc74a57bd063fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "metadata": {
   "interpreter": {
    "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
