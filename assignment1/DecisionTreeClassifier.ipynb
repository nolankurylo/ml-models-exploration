{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SENG 474\n",
    "# Assignment 1 - Problem 2\n",
    "# Nolan Kurylo\n",
    "# V00893175\n",
    "\n",
    "To execute notebook, ensure ALL cells are run from top to bottom (since imports/df creation are only called once)\n",
    "\n",
    "\n",
    "References:\n",
    "\n",
    "1) https://pandas.pydata.org/pandas-docs/stable/reference/index.html\n",
    "\n",
    "2) https://www.python-course.eu/Decision_Trees.php\n",
    "\n",
    "3) https://medium.com/@lope.ai/decision-trees-from-scratch-using-id3-python-coding-it-up-6b79e3458de4\n",
    "\n",
    "4) https://stackoverflow.com/\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DecisionTree's structure:\n",
      "{'Education': {'High School Diploma Only': {'EthnicFemale': {'Black female': {'EthnicMale': {'Black male': {'Religion': {'Catholic': 1,\n",
      "                                                                                                                         'Christian Generic': 1}},\n",
      "                                                                                             'White male': {'Religion': {'Catholic': 1,\n",
      "                                                                                                                         'Christian Generic': 1}}}},\n",
      "                                                             'Native American female': {'Religion': {'Catholic': {'EthnicMale': {'Native American male': 1}},\n",
      "                                                                                                     'Christian Generic': 0}},\n",
      "                                                             'White female': {'Religion': {'Catholic': {'EthnicMale': {'White male': 0}},\n",
      "                                                                                           'Christian Generic': {'EthnicMale': {'Black male': 0,\n",
      "                                                                                                                                'White male': 0}},\n",
      "                                                                                           'Mormon': 0,\n",
      "                                                                                           'Other': 0,\n",
      "                                                                                           'Other Misc': 0}}}},\n",
      "               'Less than High School Diploma': {'EthnicFemale': {'Black female': 1,\n",
      "                                                                  'White female': {'Religion': {'Catholic': {'EthnicMale': {'White male': 1}},\n",
      "                                                                                                'Christian Generic': 0}}}},\n",
      "               \"Some college or associate's degree\": {'EthnicFemale': {'Black female': 1,\n",
      "                                                                       'Native American female': {'Religion': {'Catholic': {'EthnicMale': {'Native American male': 1,\n",
      "                                                                                                                                           'White male': 0}},\n",
      "                                                                                                               'Christian Generic': 0}},\n",
      "                                                                       'White female': {'Religion': {'Catholic': {'EthnicMale': {'White male': 0}},\n",
      "                                                                                                     'Christian Generic': {'EthnicMale': {'Black male': 0,\n",
      "                                                                                                                                          'White male': 0}},\n",
      "                                                                                                     'Mormon': 0,\n",
      "                                                                                                     'Other': 0}}}},\n",
      "               \"bachelor's degree or higher\": {'Religion': {'Amish': 0,\n",
      "                                                            'Catholic': {'EthnicFemale': {'Black female': 1,\n",
      "                                                                                          'White female': {'EthnicMale': {'White male': 1}}}},\n",
      "                                                            'Christian Generic': {'EthnicFemale': {'Black female': 1,\n",
      "                                                                                                   'White female': {'EthnicMale': {'White male': 0}}}},\n",
      "                                                            'Mormon': 0,\n",
      "                                                            'Other': {'EthnicMale': {'White male': {'EthnicFemale': {'White female': 0}}}}}}}}\n",
      "The training accuracy of the DecisionTree: 91.73103134938664%\n",
      "The training error of the DecisionTree: 8.26896865061336%\n",
      "\n",
      "The validation accuracy of the DecisionTree: 90.36016949152543%\n",
      "The validation error of the DecisionTree: 9.639830508474574%\n",
      "\n",
      "The stumps in the DecisionTree and the number of times they are repeated are:\n",
      "Education: 1\n",
      "High School Diploma Only: 1\n",
      "EthnicFemale: 6\n",
      "Black female: 1\n",
      "EthnicMale: 11\n",
      "Black male: 1\n",
      "Religion: 8\n",
      "White male: 2\n",
      "Native American female: 2\n",
      "Catholic: 6\n",
      "White female: 5\n",
      "Christian Generic: 3\n",
      "Less than High School Diploma: 1\n",
      "Some college or associate's degree: 1\n",
      "bachelor's degree or higher: 1\n",
      "Other: 1\n",
      "\n",
      "Maximum Depth of DecisionTree is: 8\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pprint\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "np.random.seed(1337)\n",
    "\n",
    "df = pd.read_csv('elections_clean.csv') # create dataframe of csv\n",
    "\n",
    "\n",
    "label_vector = df.pop('Democrat') # this is the target/label vector\n",
    "df['Democrat'] = label_vector # place label vector at the end of df\n",
    "feature_list = ['Education','Religion','EthnicMale','EthnicFemale'] # these will be the features in the tree\n",
    "\n",
    "\n",
    "def entropy(col):\n",
    "    \"\"\" Finds the entropy of the input column col\n",
    "    :param col: input column\n",
    "    :return: the entropy of the column (information gain)\n",
    "    \"\"\"\n",
    "    values, num_values = np.unique(col, return_counts=True) \n",
    "    entropy = 0\n",
    "    for i in range(len(values)): # calculation from lecture slides\n",
    "        entropy = entropy + (-num_values[i]/np.sum(num_values))*np.log2(num_values[i]/np.sum(num_values))\n",
    "    return entropy\n",
    "\n",
    "def conditional_entropy(dataset, col):\n",
    "    \"\"\" Finds the conditional entropy of the subset dataset for the input column col\n",
    "    :param dataset: input dataframe\n",
    "    :param col: column of dataframe to find conditional entropy of\n",
    "    :return: the conditional entropy of the column (information gain)\n",
    "    \"\"\"\n",
    "    values, num_values = np.unique(col, return_counts=True) \n",
    "    cond_entropy = 0\n",
    "    for i in range(len(values)):\n",
    "        matching_rows = dataset.loc[col==values[i]] # In the column col, find all rows with matching value values[i] \n",
    "        label_vector_subset = matching_rows['Democrat'] # return a subset of the values in the label vector\n",
    "        cond_entropy = cond_entropy + ((num_values[i]/np.sum(num_values)) * entropy(label_vector_subset) ) \n",
    "    return cond_entropy\n",
    "\n",
    "def information_gain(dataset, col): \n",
    "    \"\"\" Finds the information gain of the subset dataset for the input column col\n",
    "    :param dataset: input dataframe\n",
    "    :param col: input column\n",
    "    :return: the total entropy of the column (information gain)\n",
    "    \"\"\"\n",
    "    total_entropy = entropy(dataset['Democrat']) # get the entropy for the entire label vector column\n",
    "    cond_entropy = conditional_entropy(dataset, col)\n",
    "    information_gain = total_entropy - cond_entropy # calc info gain\n",
    "    return information_gain\n",
    "   \n",
    "\n",
    "def max_info_gain(dataset, feature_list): \n",
    "    \"\"\" Finds the feature still in the feature_list with the maximum information gain\n",
    "    :param dataset: input dataframe\n",
    "    :param feature_list: subset of features from the features in the dataset \n",
    "    :return feature with the highest information gain\n",
    "    \"\"\"\n",
    "    info_gains = {}\n",
    "\n",
    "    for feature in feature_list:\n",
    "        info_gain = information_gain(dataset, dataset[feature])\n",
    "        info_gains[feature] = info_gain\n",
    "\n",
    "    return max(info_gains, key=info_gains.get)\n",
    "\n",
    "\n",
    "def ID3(dataset, feature_list, parent=None):\n",
    "    \"\"\" Recursively builds a dictionary-structured Decision Tree Classifier based on ID3 algorithm\n",
    "    :param dataset: training set\n",
    "    :param feature_list: list of all features from the dataset\n",
    "    :param parent: most common element in the current label vector subset\n",
    "    :return: Decision Tree structured as a dictionary\n",
    "    \"\"\"\n",
    "\n",
    "    if len(np.unique(dataset[\"Democrat\"])) == 1: # if the label vector is \"pure\" (only one value in label vector)\n",
    "        return dataset[\"Democrat\"][0] # return the pure value and go back up tree\n",
    "\n",
    "    if(len(feature_list) == 0): # reached the end of the branch\n",
    "        return parent # return parent value and go back up tree\n",
    "\n",
    "   \n",
    "    curr_node = max_info_gain(dataset, feature_list) # find the feature with the max information gain and make it the curr_node in the tree\n",
    "\n",
    "    curr_node_values, num_values = np.unique(dataset[curr_node], return_counts=True) # get the values and their counts associated with the curr_node\n",
    "    \n",
    "    parent = dataset['Democrat'].mode()[0] # make parent the most common element in the current label vector subset\n",
    "    \n",
    "\n",
    "    DecisionTree = {curr_node:{}}\n",
    "\n",
    "    feature_list = list(filter(lambda feature: feature != curr_node, feature_list)) # remove the curr_node (.remove() was causing issues)\n",
    "    \n",
    "\n",
    "    for value in curr_node_values: #alue associated with the curr_node feature\n",
    "        \n",
    "        sub_dataset = dataset[dataset[curr_node] == value].reset_index(drop=True) # split data into a subset that excludes all rows of the current 'value' of the curr_node in the loop\n",
    "        DecisionTree[curr_node][value] = ID3(sub_dataset, feature_list, parent) # recursively build the tree on the new subset as the dataset\n",
    "           \n",
    "    return DecisionTree\n",
    "        \n",
    "\n",
    "def split_training_validation_sets(df):\n",
    "    \"\"\" Find 70% of the original dataset as the training set, 30% as the validaiton set\n",
    "    :param df: dataframe to be split\n",
    "    :return: training and validation splits as dataframes\n",
    "    \"\"\"\n",
    "    shuffled_dataset = df.sample(frac=1).reset_index(drop=True) # shuffle the dataset\n",
    "    \n",
    "    split_70_30 = int(df.shape[0] * 0.7) # find the index for the 70 / 30 split to split on\n",
    "    training_set = shuffled_dataset.iloc[:split_70_30].reset_index(drop=True) # 70% of dataset\n",
    "    validation_set = shuffled_dataset.iloc[split_70_30:].reset_index(drop=True) # 30% of dataset\n",
    "    return training_set,validation_set\n",
    "    \n",
    "\n",
    "def predict(validation_sample, DecisionTree): \n",
    "    \"\"\" Recursively runs through DecisionTree and returns where the input validation_sample evaluates to a 1 or 0 for its label\n",
    "    :param validation_sample: sample for the Decision tree to predict\n",
    "    :param DecisionTree: dictionary structured model\n",
    "    :return: the binary prediction - 0 or 1\n",
    "    \"\"\"\n",
    "    # decsion is made based on the already-built input DecisionTree \n",
    "    \n",
    "    for feature in list(validation_sample.keys()): # for each feature in the sample\n",
    "        \n",
    "        if feature not in list(DecisionTree.keys()): #ignore missing features\n",
    "            continue\n",
    "        try:\n",
    "            prediction = DecisionTree[feature][validation_sample[feature]] # see if node in tree is a leaf( 0 or 1) or a dict (sub tree)\n",
    "            \n",
    "            \n",
    "            if(prediction == 1 or prediction == 0): # node is a leaf, return the label value\n",
    "                \n",
    "                return prediction\n",
    "            else: # node is a subtree, recursively go down to next node in subtree\n",
    "                \n",
    "                return predict(validation_sample, prediction)\n",
    "\n",
    "        except:\n",
    "            return df['Democrat'].mean() # return \"Majority vote\"\n",
    "        \n",
    "\n",
    "\n",
    "def validate(validation_set, DecisionTree):\n",
    "    \"\"\" Finds the accuracy of the DecsionTree given an input validaetion_set and the DecisionTree\n",
    "    :param validation_set: samples for the Decision tree to predict\n",
    "    :param DecisionTree: dictionary structured model\n",
    "    :return: validation accuracy (%)\n",
    "    \"\"\"\n",
    "    \n",
    "    validation_samples = validation_set.iloc[:,:-1].to_dict(orient='records') # convert dataset to dictionary\n",
    "    \n",
    "    actual_output = validation_set.iloc[:,-1] # remove the label vector so we can see how how well the tree predicts the samples\n",
    "    num_correct = 0 # store the number of samples in the validation_set that are actually correct\n",
    "    \n",
    "    for i in range(len(validation_samples)):\n",
    "        prediciton = predict(validation_samples[i],DecisionTree) # label vector either predicted as 1 or 0 based on DecisionTree\n",
    "        \n",
    "        if(prediciton == actual_output[i]): \n",
    "            num_correct += 1 # DecisionTree predicted the sample correctly\n",
    "    \n",
    "    return (num_correct/len(validation_samples)) * 100.0 # return overall accuracy percentage\n",
    "\n",
    "\n",
    "def num_stumps(DecisionTree,  stumps = {} ): \n",
    "    \"\"\" Recursively finds the stumps in the tree and the number of times each of them appear (features and a feature's values can be stumps)\n",
    "    :param DecisionTree: dictionary structured model\n",
    "    :param stumps: dict to record number of times each stump has been seen\n",
    "    :return: stumps dictionary\n",
    "    \"\"\"\n",
    "    for feature in list(DecisionTree.keys()):\n",
    "        stump = DecisionTree[feature] # see if stump in tree is a leaf( 0 or 1) or a dict (sub tree)\n",
    "        \n",
    "        if(stump != 1 and stump != 0): # if its a subtree, increment the number of times this stump has appeared\n",
    "            if(feature in stumps): # stump has been seen before\n",
    "                stumps[feature] += 1\n",
    "            else: # stump has never been seen before\n",
    "                stumps[feature] = 1\n",
    "            \n",
    "            num_stumps(stump, stumps) # recurse down subtree to the next stump\n",
    "        \n",
    "    return stumps\n",
    "\n",
    "def depth(DecisionTree, prev_depth, stumps = {} ):\n",
    "    \"\"\" Recursively fidns the maximum depth of the DecisionTree (not including leaf nodes)\n",
    "    :param DecisionTree: dictionary structured model\n",
    "    :param prev_depth: subtree's last seen depth\n",
    "    :param stumps: dict to record number of times each stump has been seen\n",
    "    :return: the largest depth of the Decision Tree\n",
    "    \"\"\"\n",
    "    curr_depth = prev_depth # set the current depth to the subtree's last known depth\n",
    "    for feature in list(DecisionTree.keys()):\n",
    "        stump = DecisionTree[feature] # see if stump in tree is a leaf( 0 or 1) or a dict (sub tree)\n",
    "        \n",
    "        if(stump != 1 and stump != 0):# if stump is a subtree, keep recursing down to find the max depth\n",
    "            new_depth = depth(stump, prev_depth + 1, stumps) # record the depth of the subtree\n",
    "            curr_depth = max(new_depth, curr_depth) # store the larger depth of the two\n",
    "\n",
    "    return curr_depth\n",
    "        \n",
    "\n",
    "# Split dataset into training and validation\n",
    "training_set, validation_set = split_training_validation_sets(df)\n",
    "\n",
    "# Create the decision tree based on the training set\n",
    "DecisionTree = ID3(training_set, feature_list) \n",
    "print(\"The DecisionTree's structure:\")\n",
    "pprint.pprint(DecisionTree) # print the dictionary structure of the tree\n",
    "\n",
    "# Obtain the accuracy/errors of the DecisionTree\n",
    "training_acc = validate(training_set, DecisionTree)\n",
    "training_err = 100 - training_acc\n",
    "print(\"The training accuracy of the DecisionTree: \" + str(training_acc) + \"%\")\n",
    "print(\"The training error of the DecisionTree: \" + str(training_err) + \"%\")\n",
    "print()\n",
    "\n",
    "validation_acc = validate(validation_set, DecisionTree)\n",
    "validation_err = 100 - validation_acc\n",
    "print(\"The validation accuracy of the DecisionTree: \" + str(validation_acc) + \"%\")\n",
    "print(\"The validation error of the DecisionTree: \" + str(validation_err) + \"%\")\n",
    "print()\n",
    "\n",
    "# Report the number of features (also each feature's values) that are repeated in decision stumps\n",
    "stumps = num_stumps(DecisionTree)\n",
    "print(\"The stumps in the DecisionTree and the number of times they are repeated are:\")\n",
    "for stump in stumps:\n",
    "    print(stump + \": \" + str(stumps[stump]))\n",
    "print()\n",
    "\n",
    "\n",
    "# Report the maximum depth of the decision tree (depth of root = 1)\n",
    "max_depth = depth(DecisionTree, 1)\n",
    "print(\"Maximum Depth of DecisionTree is: \" + str(max_depth))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "name": "python390jvsc74a57bd063fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "metadata": {
   "interpreter": {
    "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
