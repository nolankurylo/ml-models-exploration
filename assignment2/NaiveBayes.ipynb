{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SENG 474\n",
    "# Assignment 2 - Problem 3\n",
    "# Nolan Kurylo\n",
    "# V00893175\n",
    "To execute notebook, ensure ALL cells are run from top to bottom \n",
    "\n",
    "The Naive Bayes classifier required use of the following formulas:\n",
    "\n",
    "A Priori Probabilities:\n",
    "$$ \n",
    "P(spam) = \\frac{\\#\\ spam\\ messages}{\\#\\ total\\ messages}\n",
    "$$\n",
    "$$\n",
    "P(ham) = \\frac{\\#\\ ham\\ messages}{\\#\\ total\\ messages}  \n",
    "$$\n",
    "\n",
    "Conditional Probabilities:\n",
    "$$ \n",
    "P(w_i|spam) = \\frac{\\#\\ spam\\ messages\\ containing\\ word\\ w_i}{\\#\\ spam\\ messages} \n",
    "$$\n",
    "$$\n",
    "P(w_i|ham) = \\frac{\\#\\ ham\\ messages\\ containing\\ word\\ w_i}{\\#\\ ham\\ messages} \n",
    "$$\n",
    "\n",
    "A Posteriori Probabilities:\n",
    "\n",
    "$$\n",
    "P(spam|w_1,w_2,...,w_n) = P(spam) \\prod_i^n P(w_i|spam) \n",
    "$$\n",
    "$$\n",
    "P(ham|w_1,w_2,...,w_n) = P(ham) \\prod_i^n P(w_i|ham) \n",
    "$$\n",
    "\n",
    "Bayes Decision Function:\n",
    "\n",
    "If:\n",
    "$$\n",
    "P(spam|w_1,w_2,...,w_n) > P(ham|w_1,w_2,...,w_n)\n",
    "$$\n",
    "Decide: Spam\n",
    "\n",
    "Otherwise \n",
    "$$\n",
    "P(spam|w_1,w_2,...,w_n) < P(ham|w_1,w_2,...,w_n)\n",
    "$$\n",
    "Decide: Ham\n",
    "\n",
    "References\n",
    "1) https://www.kdnuggets.com/2020/07/spam-filter-python-naive-bayes-scratch.html\n",
    "2) https://towardsdatascience.com/na%C3%AFve-bayes-spam-filter-from-scratch-12970ad3dae7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...(takes ~20-25 seconds)\n",
      "\n",
      "Training Accuracy = 99.53846153846155%\n",
      "Training Error = 0.4615384615384528%\n",
      "\n",
      "Validation Accuracy = 90.07177033492823%\n",
      "Validation Error = 9.928229665071768%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(1337)\n",
    "\n",
    "\n",
    "# Slighty modified function I made for A1\n",
    "def split_training_validation_sets(df): \n",
    "    \"\"\" Find 70% of the original dataset as the training set, 30% as the validaiton set\n",
    "    :param df: dataframe to be split\n",
    "    :return: training and validation splits as dataframes\n",
    "    \"\"\"\n",
    "    shuffled_dataset = df.sample(frac=1).reset_index(drop=True) # shuffle the dataset\n",
    "    \n",
    "    split_70_30 = int(df.shape[0] * 0.7) # find the index for the 70 / 30 split to split on\n",
    "\n",
    "    training_set = shuffled_dataset.iloc[:split_70_30].reset_index(drop=True) # 70% of dataset\n",
    "    training_set['MSG'] = training_set['MSG'].str.lower().replace('[^\\s\\da-zA-Z]', ' ', regex = True) # replace all no alphanumeric characters with a space and convert to lowercase\n",
    "    training_set['MSG'] = training_set['MSG'].str.split()\n",
    "\n",
    "    validation_set = shuffled_dataset.iloc[split_70_30:].reset_index(drop=True) # 30% of dataset\n",
    "    validation_set['MSG'] = validation_set['MSG'].str.lower().replace('[^\\s\\da-zA-Z]', ' ', regex = True) # replace all no alphanumeric characters with a space and convert to lowercase\n",
    "    validation_set['MSG'] = validation_set['MSG'].str.split()\n",
    "\n",
    "    return training_set, validation_set\n",
    "\n",
    "def fit(train_X, train_y): \n",
    "    \"\"\" Finds the conditional probabilities of each word i given if it was a spam or ham message as well as the a priori probabilities for spam and ham across the dataset\n",
    "    :param train_X: training set\n",
    "    :param train_y: training label vector\n",
    "    :return: P(wi|spam), P(wi|ham), P(spam), P(ham)\n",
    "    \"\"\"\n",
    "    print(\"Training...(takes ~20-25 seconds)\")\n",
    "\n",
    "    words = [] # list of all unique words in train_X\n",
    "    for index, bag in train_X.iteritems(): #find all unique words in train_X and add each to a list\n",
    "        for word in bag: # each row is a bag of words\n",
    "            if word not in words: # make sure unique words added to list only\n",
    "                words.append(word)\n",
    "\n",
    "    word_counts = {} # dict for each word in the dataset that contains an array of counts for the number of times it appears in each row (bag)\n",
    "    Pwi_spam = {} # dict for each word in the dataset that will eventually be given a conditional probability of that word appearing given that it is a spam message\n",
    "    Pwi_ham= {} # dict for each word in the dataset that will eventually be given a conditional probability of that word appearing given that it is a ham message\n",
    "\n",
    "    num_rows = len(train_X)\n",
    "    \n",
    "    for word in words: \n",
    "        word_counts[word] = [0] * num_rows # for each unique word in the dataset, initialize the number of times the word is present to 0 for each row of training set\n",
    "        Pwi_spam[word] = 0 # for each unique word in the dataset, initialize its conditional probaility of a word being a spam word given that it is spam\n",
    "        Pwi_ham[word] = 0 # for each unique word in the dataset, initialize its conditional probaility of a word being a ham word given that it is ham\n",
    "    \n",
    "    for index, bag in train_X.iteritems(): # for each bag in the training set, increment the number of times each unique word is present\n",
    "        for word in bag:\n",
    "            word_counts[word][index] += 1\n",
    "\n",
    "    train_X = pd.DataFrame(word_counts) # transform word counts matrix to a dataframe where each unique word is a column and each row is the number of times that column word is present mapping to each row in train_X\n",
    "\n",
    "    spams = train_X.iloc[np.where(train_y == 'spam')[0]] # get all spam messages\n",
    "    num_spam = len(spams)\n",
    "\n",
    "    hams = train_X.iloc[np.where(train_y == 'ham')[0]] # get all ham messages\n",
    "    num_ham = len(hams)\n",
    "\n",
    "    Pspam = num_spam / num_rows # P(spam) = # spam messages / total # messages -> a priori\n",
    "    Pham = num_ham / num_rows # P(ham) = # ham messages / total # messages -> a priori\n",
    "\n",
    "    for word in words: #find conditional probability of each word given than it is ham or spam\n",
    "        Pwi_spam[word] = spams[word].sum() / num_spam # P(wi|spam) = #spam messages containing word / # spam messages\n",
    "        Pwi_ham[word] = hams[word].sum() / num_ham # P(wi|ham) = #ham messages containing word / # ham messages\n",
    "\n",
    "    return Pwi_spam, Pwi_ham, Pspam, Pham\n",
    "\n",
    "\n",
    "def predict(test_X, test_y, Pwi_spam, Pwi_ham, Pspam, Pham): \n",
    "    \"\"\" Finds the accuarcy of the naive bayes classifier by calculating the a posterior probabilities for each bag of words\n",
    "    :param test_X: testing set\n",
    "    :param test_y: testing label vector\n",
    "    :param Pwi_spam: P(wi|spam)\n",
    "    :param Pwi_ham: P(wi|ham)\n",
    "    :param Pspam: P(spam)\n",
    "    :param Pham: P(ham)\n",
    "    :return: accuracy (%)\n",
    "\n",
    "    \"\"\"\n",
    "    num_correct = 0 # keep track of number of correctly predicted messages\n",
    "    for index, bag in test_X.iteritems():\n",
    "        Pspam_wi = [] # P(spam|w1,w2,...,wn) = P(spam) * mult[P(wi|spam)]\n",
    "        Pham_wi = [] # P(ham|w1,w2,...,wn) = P(ham) * mult[P(wi|ham)]\n",
    "\n",
    "        actual = test_y.loc[index] #spam or ham\n",
    "        predicted = \"\" \n",
    "        for word in bag: \n",
    "            if word in Pwi_spam:\n",
    "                Pspam_wi.append(Pwi_spam[word]) # add all P(wi|ham) for the current bag to a list\n",
    "            if word in Pwi_ham: \n",
    "                Pham_wi.append(Pwi_ham[word]) # add all P(wi|ham) for the current bag to a list\n",
    "\n",
    "        # Find A posteriori probabilities for given bag\n",
    "        Pspam_w = Pspam * np.prod(Pspam_wi) # P(spam) mult[P(wi|spam)]\n",
    "        Pham_w = Pham * np.prod(Pham_wi) # P(ham) mult[P(wi|ham)]\n",
    "\n",
    "        if(Pspam_w > Pham_w ): # decision function for spam\n",
    "            predicted = \"spam\"\n",
    "        elif(Pham_w > Pspam_w): # decision function for ham\n",
    "            predicted = \"ham\"\n",
    "\n",
    "        if(actual == predicted):\n",
    "            num_correct += 1\n",
    "\n",
    "    num_rows = len(test_X)\n",
    "    accuracy = (num_correct / num_rows) * 100\n",
    "    return accuracy\n",
    " \n",
    "# MAIN Program\n",
    "\n",
    "df = pd.read_csv('SMSSpamCollection', sep='\\t', names=['TARGET', 'MSG']) # captilized incase these names are present in the csv (csv will be converted to lower case)\n",
    "\n",
    "training_set, validation_set = split_training_validation_sets(df)\n",
    "\n",
    "train_X = training_set['MSG']\n",
    "train_y = training_set['TARGET']\n",
    "\n",
    "test_X = validation_set['MSG']\n",
    "test_y = validation_set['TARGET']\n",
    "\n",
    "Pwi_spam, Pwi_ham, Pspam, Pham = fit(train_X, train_y) # train the naive bayes classifier using the training set\n",
    "\n",
    "training_acc = predict(train_X, train_y, Pwi_spam, Pwi_ham, Pspam, Pham) # validate the naive bayes classifier on the validation test\n",
    "training_err = 100 - training_acc\n",
    "\n",
    "validation_acc = predict(test_X, test_y, Pwi_spam, Pwi_ham, Pspam, Pham) # validate the naive bayes classifier on the validation test\n",
    "validation_err = 100 - validation_acc\n",
    "\n",
    "print()\n",
    "print(\"Training Accuracy = \" + str (training_acc)  + \"%\")\n",
    "print(\"Training Error = \" + str (training_err) + \"%\")\n",
    "print()\n",
    "print(\"Validation Accuracy = \" + str (validation_acc)  + \"%\")\n",
    "print(\"Validation Error = \" + str (validation_err) + \"%\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "63fd5069d213b44bf678585dea6b12cceca9941eaf7f819626cde1f2670de90d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
